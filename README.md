# **Expanding "The Power of Noise: Redefining Retrieval for RAG Systems"**

**This is a repository that expands on the studies initiated by Florin in his git repository called The-Power-of-Noise down below.**

* Link for his original work: https://github.com/florin-git/The-Power-of-Noise

* Link for his paper: [The Power of Noise: Redefining Retrieval for RAG Systems](https://dl.acm.org/doi/10.1145/3626772.3657834)

# **How to Download the data**
Since most of data is from the previous experiment in the Power of Noise paper, most of data is downloaded directly from there. But some files are downloaded from the Driver provided by Florin in his work

* **Command to download the data:** Just run the command python3 src/run_all_experiments.py

**What this command is going to do:** 
* This command will download all required files into a folder called data/downloaded_files.
* Then this command will automatically create a bunch of folders used with the main code
* Then, this command will also automatically move all the downloaded files to their correct folder newlly created.

**Command to run the whole application:** ./src/run_experiments.sh --all all

# **Overview**

This work originally investigates Retrieval-Augmented Generation (RAG) systems by evaluating how retrieval strategies, contextual document configurations, and data noise impact the accuracy of Large Language Models (LLMs). The central focus is on combining retrieval mechanisms like Contriever and BM25 with robust experimental setups to test the effects of gold documents, distractors, and random corpora on response generation.

Building on this foundation, we aim to reproduce and expand upon these experiments, incorporating additional techniques like clustering (K-means) and RAG-Fusion to further explore how document selection and organization can influence model performance under diverse scenarios.

# **Original Work and Expansion Goals**

**Original Work**

The original research emphasized the following elements:

**1 - Dataset**:
* The Natural Questions (NQ) dataset was used to provide queries and gold documents, filtered for compatibility with retrieval models and LLM input constraints.

**2 - Retrieval Models:**
* Contriever and BM25 were employed to rank documents based on dense and sparse retrieval mechanisms, respectively.
* Precomputed retrieval results stored document indices and similarity scores for subsequent experiments.

**3 - Context Configurations:**
* Experiments tested how introducing gold, distracting, and random documents influenced response generation.
* Contexts were evaluated for robustness to noise and arrangement, such as positioning the gold document at specific locations.

**4 - Evaluation:**
* Responses were assessed using accuracy, where a response was deemed correct if it contained at least one predefined correct answer.

**Proposed Expansion**

The expanded work retains the foundation of the original study but introduces new experimental setups and advanced methods to deepen the understanding of RAG systems' behavior under different conditions.

**Key Additions:**

**1 - Baseline (Naive RAG):**

Establish a baseline using the original retrieval mechanisms (Contriever/BM25) without further processing of retrieved documents.

**2 - Clustering with K-means:**

Apply K-means clustering to the retrieved documents to group them based on semantic similarity before feeding them into the RAG system.

**3 - RAG-Fusion:**

Aggregate documents using RAG-Fusion, a method to combine multiple retrieved results, and analyze its impact with and without additional 
clustering.

**4 - Intermediate Categories:**

Use the intermediate categories generated by RAG-Fusion as clusters directly, bypassing the need for K-means.

**5 - Impact of Noise:**

Extend each experiment by injecting random or nonsensical documents into the context to measure robustness against noise.

# *Proposed Experiments and Data Requirements*

There will be 4 experiments

**Experiment 0: Naive RAG Baseline**

* **Goal:** Establish a baseline by using BM25/Contriever retrieval results without further processing.
* **Description:** Use retrieval results to create context for RAG without applying clustering or fusion. This serves as a comparison point for more advanced methods.
* **Files Required:**
    - Dataset: datasets/10k_train_dataset.json, datasets/test_dataset.json.
    - Corpus: corpus/corpus.json.
    - Retrieval Results: Either retrieval_results/bm25_test_search_results_at250.pkl or retrieval_results/contriever_search_results_at150.pkl.

**Experiment 1: K-means Clustering on Retrieved Documents**

* **Goal:** Apply K-means clustering to the documents retrieved by Contriever/BM25 to group them by semantic similarity.
* **Description:** Select documents from the most relevant clusters for RAG, aiming to determine whether clustering improves retrieval-based context creation.
* **Files:** Same as Experiment 0.

**Experiment 2: RAG-Fusion Followed by K-means**

* **Goal:** Use RAG-Fusion to aggregate retrieved documents and then apply K-means clustering for refinement.
* **Description:** This experiment evaluates whether clustering fused contexts provides better accuracy compared to clustering raw retrieved documents.
* **Files:** Same as Experiment 0.

**Experiment 3: Intermediate Categories from RAG-Fusion**

* **Goal:** Use the intermediate categories generated by RAG-Fusion as clusters directly, bypassing K-means clustering.
* **Description:** This experiment assesses the utility of RAG-Fusionâ€™s built-in document categorization for organizing retrieved documents.
* **Files:** Same as Experiment 0.

**Noise Variations:**

For all experiments above, repeat with random and distractor documents injected into the context to evaluate noise robustness:

* **Files:**
    - Random Documents: random_results/10k_random_results_at60.pkl, random_results/10k_other_random_results_at60.pkl
    - Nonsensical Documents: random_results/nonsense_random_results.pkl
* **Description:** Compare results between contexts with clean and noisy data to assess the impact of noise on model performance.    

**Evaluation**

* **Metrics:**
    - **Accuracy:** A response is correct if it contains at least one predefined correct answer.
    - **Noise Sensitivity:** Measure the degradation in performance when noisy documents are introduced into the context.
* **Scripts:**
Use the script src/read_generation_results.py to compute metrics for each experiment.

**Conclusion**

This expanded study builds on the original "The Power of Noise" framework by introducing advanced methods like K-means clustering and RAG-Fusion. Through systematic experimentation, it aims to provide actionable insights into optimizing RAG systems for diverse, noisy, and dynamic real-world scenarios. By rigorously evaluating each method under controlled conditions, the work advances the understanding of how data organization and retrieval techniques affect RAG performance.